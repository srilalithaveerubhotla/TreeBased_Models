{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gradient_Boosting_Scratch.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOkldjQuG4f/LiouORp7CbX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilalithaveerubhotla/TreeBased_Models/blob/main/Gradient_Boosting_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIUu1DS29s2E"
      },
      "source": [
        "\n",
        "### Gradient Boosting Algorithm Steps\n",
        "\n",
        "```\n",
        "Step 1: Consider all (or a sample ) the data points to train a highly biased model.\n",
        "Step 2: Calculate residuals (errors) for each data point.\n",
        "Step 3: Train another model with the same data points and corresponding residuals (errors) as class labels.\n",
        "Step 4: Repeat Step 2 & Step 3 ( for n iterations).\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o02ZuQic3yOj"
      },
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from fastai.imports import *\n",
        "from IPython.display import display\n",
        "from sklearn import metrics\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import clear_output\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL3grwVD4srQ"
      },
      "source": [
        "def std_agg(cnt, s1, s2): return math.sqrt((s2/cnt) - (s1/cnt)**2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ccY5INs4uql"
      },
      "source": [
        "class DecisionTree():\n",
        "    def __init__(self, x, y, idxs = None, min_leaf=2):\n",
        "        if idxs is None: idxs=np.arange(len(y))\n",
        "        self.x,self.y,self.idxs,self.min_leaf = x,y,idxs,min_leaf\n",
        "        self.n,self.c = len(idxs), x.shape[1]\n",
        "        self.val = np.mean(y[idxs])\n",
        "        self.score = float('inf')\n",
        "        self.find_varsplit()\n",
        "        \n",
        "    def find_varsplit(self):\n",
        "        for i in range(self.c): self.find_better_split(i)\n",
        "        if self.score == float('inf'): return\n",
        "        x = self.split_col\n",
        "        lhs = np.nonzero(x<=self.split)[0]\n",
        "        rhs = np.nonzero(x>self.split)[0]\n",
        "        self.lhs = DecisionTree(self.x, self.y, self.idxs[lhs])\n",
        "        self.rhs = DecisionTree(self.x, self.y, self.idxs[rhs])\n",
        "\n",
        "    def find_better_split(self, var_idx):\n",
        "        x,y = self.x.values[self.idxs,var_idx], self.y[self.idxs]\n",
        "        sort_idx = np.argsort(x)\n",
        "        sort_y,sort_x = y[sort_idx], x[sort_idx]\n",
        "        rhs_cnt,rhs_sum,rhs_sum2 = self.n, sort_y.sum(), (sort_y**2).sum()\n",
        "        lhs_cnt,lhs_sum,lhs_sum2 = 0,0.,0.\n",
        "\n",
        "        for i in range(0,self.n-self.min_leaf-1):\n",
        "            xi,yi = sort_x[i],sort_y[i]\n",
        "            lhs_cnt += 1; rhs_cnt -= 1\n",
        "            lhs_sum += yi; rhs_sum -= yi\n",
        "            lhs_sum2 += yi**2; rhs_sum2 -= yi**2\n",
        "            if i<self.min_leaf or xi==sort_x[i+1]:\n",
        "                continue\n",
        "\n",
        "            lhs_std = std_agg(lhs_cnt, lhs_sum, lhs_sum2)\n",
        "            rhs_std = std_agg(rhs_cnt, rhs_sum, rhs_sum2)\n",
        "            curr_score = lhs_std*lhs_cnt + rhs_std*rhs_cnt\n",
        "            if curr_score<self.score: \n",
        "                self.var_idx,self.score,self.split = var_idx,curr_score,xi\n",
        "\n",
        "    @property\n",
        "    def split_name(self): return self.x.columns[self.var_idx]\n",
        "    \n",
        "    @property\n",
        "    def split_col(self): return self.x.values[self.idxs,self.var_idx]\n",
        "\n",
        "    @property\n",
        "    def is_leaf(self): return self.score == float('inf')\n",
        "    \n",
        "    def __repr__(self):\n",
        "        s = f'n: {self.n}; val:{self.val}'\n",
        "        if not self.is_leaf:\n",
        "            s += f'; score:{self.score}; split:{self.split}; var:{self.split_name}'\n",
        "        return s\n",
        "\n",
        "    def predict(self, x):\n",
        "        return np.array([self.predict_row(xi) for xi in x])\n",
        "\n",
        "    def predict_row(self, xi):\n",
        "        if self.is_leaf: return self.val\n",
        "        t = self.lhs if xi[self.var_idx]<=self.split else self.rhs\n",
        "        return t.predict_row(xi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKIyLxWy9Kdo"
      },
      "source": [
        "x = np.arange(0,50)\n",
        "x = pd.DataFrame({'x':x})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeNw5vd-9O5v"
      },
      "source": [
        "# just random uniform distributions in differnt range\n",
        "\n",
        "y1 = np.random.uniform(10,15,10)\n",
        "y2 = np.random.uniform(20,25,10)\n",
        "y3 = np.random.uniform(0,5,10)\n",
        "y4 = np.random.uniform(30,32,10)\n",
        "y5 = np.random.uniform(13,17,10)\n",
        "\n",
        "y = np.concatenate((y1,y2,y3,y4,y5))\n",
        "y = y[:,None]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHWo8VSI9RUd"
      },
      "source": [
        "x.shape, y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qkmq9muN9TyH"
      },
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(x,y, 'o')\n",
        "plt.title(\"Scatter plot of x vs. y\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYad-TaA6jmb"
      },
      "source": [
        "xi = x # initialization of input\n",
        "yi = y # initialization of target\n",
        "# x,y --> use where no need to change original y\n",
        "ei = 0 # initialization of error\n",
        "n = len(yi)  # number of rows\n",
        "predf = 0 # initial prediction 0\n",
        "\n",
        "for i in range(30): # loop will make 30 trees (n_estimators). \n",
        "    tree = DecisionTree(xi,yi) # DecisionTree scratch code can be found in shared github/kaggle link. \n",
        "                               # It just create a single decision tree with provided min. sample leaf\n",
        "    tree.find_better_split(0)  # For selected input variable, this splits (<n and >n) data so that std. deviation of \n",
        "                               # target variable in both splits is minimum as compared to all other splits\n",
        "    \n",
        "    r = np.where(xi == tree.split)[0][0]   #  finds index where this best split occurs\n",
        "    \n",
        "    left_idx = np.where(xi <= tree.split)[0] # index lhs of split\n",
        "    right_idx = np.where(xi > tree.split)[0] # index rhs of split\n",
        "    \n",
        "    predi = np.zeros(n)\n",
        "    np.put(predi, left_idx, np.repeat(np.mean(yi[left_idx]), r))  # replace left side mean y\n",
        "    np.put(predi, right_idx, np.repeat(np.mean(yi[right_idx]), n-r))  # right side mean y\n",
        "    \n",
        "    predi = predi[:,None]  # make long vector (nx1) in compatible with y\n",
        "    predf = predf + predi  # final prediction will be previous prediction value + new prediction of residual\n",
        "    \n",
        "    ei = y - predf  # needed originl y here as residual always from original y    \n",
        "    yi = ei # update yi as residual to reloop\n",
        "    \n",
        "    \n",
        "    # plotting after prediction\n",
        "    xa = np.array(x.x) # column name of x is x \n",
        "    order = np.argsort(xa)\n",
        "    xs = np.array(xa)[order]\n",
        "    ys = np.array(predf)[order]\n",
        "    \n",
        "    #epreds = np.array(epred[:,None])[order]\n",
        "\n",
        "    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize = (13,2.5))\n",
        "\n",
        "    ax1.plot(x,y, 'o')\n",
        "    ax1.plot(xs, ys, 'r')\n",
        "    ax1.set_title(f'Prediction (Iteration {i+1})')\n",
        "    ax1.set_xlabel('x')\n",
        "    ax1.set_ylabel('y / y_pred')\n",
        "\n",
        "    ax2.plot(x, ei, 'go')\n",
        "    ax2.set_title(f'Residuals vs. x (Iteration {i+1})')\n",
        "    ax2.set_xlabel('x')\n",
        "    ax2.set_ylabel('Residuals')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOSQcBYwEzz5"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "\n",
        "Errors are not changing much after 20th iteration and pattern\n",
        " in residuals is also removed. Residuals look distributed around the mean\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFlTIcLXE287"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6T4MZtE6mFE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}